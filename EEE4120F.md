![[golden_gate_bridge.png]]
## Golden Measure
- A sequential solution that you develop as the yard stick
- A solution that may run slowly, isn't optimised but you know it gives (numerically speaking) excellent results
- Its not about computation time but accuracy
## Sequential / Serial
A non-parallelized code solution
## Speed-up
- $\text{Speed-up} = \frac{T_{p1}}{T_{p2}}$ . $T_{p1}$ = run-time of original (or non-optimized) program
- $T_{p2}$ = run-time of optimised prograam
- Best practise: run the program more than once so as to warm up the system
<<<<<<< HEAD
=======


12/03/2024
## Verification and Validation (V&V)
- Verification (dev end)
  - “Are we building the product right?”
  - Have we made what we understood we wanted to make?
  - Does the product satisfy its specifications?
- Validation (user end)
  - “Are we building the right product?”
  - Does the product satisfy the users’ requirements
- Verification before validation (at least in duress)…
While it would be nice to validate (seeing that the users are happy) before verifying (checking the specs), doing so would mean your final design might not match the specifications (which could open the door to legal problems). Obviously this often doesn’t happen because in practice you want to make sure the client is happy and there might not be time for proper validation.
## Verification before validation
- The RC engineer (i.e., you) are effectively designing both custom hardware and custom
software for the RC platform
- Before attempting to make claims about the validity of your system, it’s usually best practice to establish your own (or team’s) confidence in what your system is doing, i.e. be sure that:
  - The custom hardware working;
  - The software implementation is doing what it was designed to do; and
  - The custom software runs reliably on the custom hardware.
## Verification
- Checking plans, documents, code, requirements and specifications
- Is everything that you need there?
- Algorithms/functions working properly?
- Done during phase interval (e.g., design => implementation)
- Activities:
  - Review meetings, walkthroughs, inspections
  - Informal demonstrations
## Commonly used verification methods
1. Dual processing, producing two result sets
  1. One version using PC & simulation only;
  2. Other version including RC platform
2. Assume the PC version is the correct one (i.e., the gold measure)
3. Correlate the results to establish correlation coefficients
(complex systems may have many different sets of possibly multidimensional data that need to be compared)
The correlation coefficients can be used as a kind of ‘confidence factor’
## Validation
- Testing of the whole product / system
- Input: checklist of things to test or list of issues that need to have been provided/fixed
- Towards end of project
- Activities:
  - Formal demonstrations
  - Factory Acceptance Test
## Testing and Correctness proofs
- Testing
  - Generally refers to aspects of dynamic validation in which a program is executed and the results analysed
- Correctness proofs / formal verification
  - More a mathematical approach
  - Exhaustive test => specification guaranteed correct
  - Formal verification of hardware is especially relevant to RC. Formal methods include:
    -Model checking / state space exploration
    -Use of linear temporal logic and computational tree logic
    -Mathematical proof (e.g. proof by induction)
## Amdahl’s Law Insert graph on page 15 here

## Essentials of Amdahl’s Law
- Be aware that a computer program to run on a parallel computer * pretty much always has a part that is sequential, which can run on only one core, and a part that is parallel, that can be split between available cores
- But, let’s make things more fun (and hope you then understand Amdahl’s better) by proceeding to video linked on next slide.
- Comments on slide 19 elaborates further.
*well, we’re thinking here computers with one or more CPUs for their processing

## Amdahl’s Law
- Define f as: fraction of computation that can be parallelized (ignoring scheduling overhead)
- Then (1 -f ) is the fraction that is sequential
- Define n = no. processors for parallel case
- The maximum speed-up achievable is:
$$\text{Speedup}_\text{parallel} = \frac{1}{(1-f)+\frac{f}{n}}$$
Should be able to remember this formula for exams
### Alternate Representation
P = expected performance improvement
$E^u$ = Execution time on a uniprocessor (serial)
$E^p$ = Execution time on a number of processors (parallel)
n = number of processors
S = fraction of time spent in the sequential time
$$P = \frac{E^u}{E^p} = \frac{E^u}{SE^u+\frac{(1-s)}{N}E^u} = \frac{1}{s+\frac{1-S}{N}}$$




# Lecture 6
# Computation Methods

| Hardware                                                                                                                                                                                    | Reconfigurable Computer                                                                                                                                                                                                                                    | Software Processor                                                                                                                                                                                                                |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Advantages:<br>• High speed & performance<br>• Efficient (possibly<br>lower power than idle<br>processors)<br>• Parallelizable<br>Drawbacks:<br>• Expensive<br>• Static (cannot change)<br> | e.g. IBM Blade, FPGA-based<br>computing platform<br>Advantages:<br>• Faster than software alone<br>• More flexible than software<br>• More flexible than hardware<br>• Parallelizable<br>Drawbacks:<br>• Expensive<br>• Complex<br>(both s/w<br>& h/w)<br> | e.g. PC, embedded<br>software on<br>microcontroller<br>Advantages:<br>• Flexible<br>• Adaptable<br>• Can be much<br>cheaper<br>Drawbacks:<br>• The hardware is static<br>• Limit of clock speed<br>• Sequential<br>processing<br> |
# Mainstream parallel computing
- Most server class machines today are:
	- PC class SMP’s (Symmetric Multi-Processors *)
	- 2, 4, 8 processors - cheap
	- Run Windows & Linux
- Delux SMP’s
	- 8 to 64 processors
	- Expensive:
	- 16-way SMP costs ≈ 4 x 4-way SMPs
- Applications: Databases, web servers, internet commerce / OLTP (online transaction processing)
- Newer applications: technical computing, threat analysis, credit card fraud...
![[SMP.png]]
* Also termed “Shared Memory Processor” (but you might get 0 for giving this alternate term in a test)
# Large scale parallel computing systems
- Hundreds of processors, typically as SMPs clusters
- Traditionally
	- Often custom built with government funding (costly! 10 to 100 million USD)
	- National / international resource
	- Total sales tiny fraction of PC server sales
- Few independent software developers
- Programmed by small set of majorly smart people
- Later trends
	- Cloud systems
	- Users from all over
	- E.g. Amazon EC, Microsoft Azure
- Some application examples
	- Code breaking (CIA, FBI)
	- Weather and climate modeling / prediction
	- Pharmaceutical – drug simulation, DNA modeling and drug design
	- Scientific research (e.g., astronomy, SETI*)
	- Defense and weapons development
- Large-scale parallel systems are often used for modelling
* This one has changed, SETI@home which used many volunteers across the world doing processing was large scale parallel but not a single computing system… so this example isn’t really valid for this case. Furthermote, SETI@home is currently hibernating.
# Classic techniques for parallel programming*
- Single Program Multiple Data (SPMD)
	- Consider it as running the same program, on different data inputs, on different computers (possibly) at the same time
- Multiple Program Multiple Data (MPMD)
	- Consider this one as running the same program with different parameters settings, or recompiling the same code with different sections of code included (e.g., uisng #ifdef and #endif to do this)
- Following this approach performance statistics can be gathered (without necessarily any parallel code being written) and then evaluated after the effect to deem the feasibility of implementing a parallel (e.g. actual pthreads version) of the program.
Can consider the SPSD (i.e. Single Program Single Data) as the case where the whole program is run once on all the data.
# Terms (reminders)
- Observed speedup =
$$
\frac{\text{Wallclock time initial version
}}{\text{Wallclock time refined version}} =\frac{\text{Wallclock time sequential (or gold)}}{\text{Wallclock time parallel version}}
$$
- Parallel overhead:
- Amount of time to coordinate parallel tasks (excludes time doing useful work).
- Parallel overhead includes operations such as:
	Task/co-processor start-up time, 	Synchronizations, communications,
	parallelization libraries (e.g., OpenMP, 	Pthreads.so), tools, operating system, task 	termination and clean-up time

The parallel overhead of the lazy parallel model could clearly be extreme, considering that is would rely on manual intervention to (e.g.) partition and prepare the data before the program runs.
## Some terms
- Embarrassingly Parallel
	- Simultaneously performing many similar, independent tasks, with little to no coordination between tasks.
- Massively Parallel
	- Hardware that has very many processors (execution of parallel tasks). Can consider this classification of 100 000+ parallel tasks.
- { Stupidly Parallel }
	- While this isn’t really an official term it typically relates to instances where a big (and possibly very complex) coding effort is put into developing a solution that in practice has negligible savings or worse is a whole lot slower (and possibly more erroneous/buggy) than if it was just left as a simpler sequential implementation. 
### Partitioned memory
![[partitioned_memory.png]]
### Interlaced* memory
![[Interlaced_memory.png]]
## Memory Partitioning Terms
![[memoryPartitioningTerm.png]]
## Flynn’s taxonomy
- Flynn’s (1966) taxonomy was developed as a means to classify parallel computer architectures
- Computer system can be fit into one of the following four forms:
![[FlynsTaxonomy.png]]
Not to be confused with the terms of “Single Program Multiple Data (SPMD)” and “Multiple Program Multiple Data (MPMD)” mentioned earlier.
#### Single Instruction Single Data (SISD)
- This is (obviously) the classic von Neumann Computer: serial (not parallel) computer, e.g.:
	- Old style single core PC CPUs, e.g. i486
- Single instruction →
	- One instruction stream acted on by the CPU during any one clock cycle
- Single data →
	- Only one input data stream for any one clock cycle
- Deterministic execution
![[SISD.png]]
#### Single Instruction Multiple Data (SIMD)
- A form of parallel computer
	- Early supercomputers used this model first
	- Nowadays it has become common – e.g., used in modern computers on GPUs
- Single instruction →
	- All processing units execute the same instruction for any given clock cycle
- Multiple data →
	- Each processing unit can operate on a different data element
![[SIMD.png]]
#### Single Instruction Multiple Data (SIMD)
- Runs in lockstep (i.e., all elements synchronized)
- Works well for algorithms with a lot of regularity; e.g. graphics processing.
- Two main types:
	- Processor arrays
	- Vector pipelines
- Still highly deterministic (know the same operation is applied to specific set of data – but more data to keep track of per instruction)
#### Single Instruction Multiple Data (SIMD) Examples
- Vector pipelines
	- IBM 9000, Cray X-MP,
	Fujitsu vector processor,
	NEC SX-2, Hitachi S820, ETA10
- Processor arrays
	- Thinking Machine CM2,
	MasPar MP-1 & MP-2,
	ILLIAC IV
- Graphics processor units usually use SIMD
#### Multiple Instruction Single Data (MISD)
- Single data stream fed into multiple processing units
- Each processing unit works on data independently via independent instruction streams
- Few actual examples of this class of parallel computer have ever existed
#### Multiple Instruction Single Data (MISD) Example
- Possible uses? Somewhat intellectual?
	- Maybe redundant! (see next slide)
- Possible example application:
	- Different set of signal processing operations working the same signal stream
![[simMinMax.png]]
#### Multiple Instruction Multiple Data (MIMD)
- The most common type of parallel computer (most late model computers, e.g. Intel Core Duo, in this category)
- Multiple Instruction →
	- Each processor can be executing a different instruction stream
- Multiple Data →
	- Every processor may be working with a different data stream
- Execution can be asynchronous or synchronous; non-deterministic or deterministic
- Examples
	- Many of the current supercomputers
	- Networked parallel computer clusters
	- SMP computers
	- multi-core PCs
- MIMD architectures could include all the other models. e.g.,
	- SISD – just one CPU active, others running NOP
	- SIMD – all CPUs load the same instruction but apply to different data
	- MISD – all CPUs load different instructions but apply it to the same data
# Maximum Effective Parallelism
## Significant Tradeoff:
### Infrastructure & Setup Cost vs. Effective Parallelism
Consideration for
- hardware
- software
### Maximum Effective Parallelism:
This refers to the point of the number of cores / amount of parallelism
beyond which additional parallelism provides no further benefit or (worse)
may reduce performance.
### Maximum Effective Parallelism:
This refers to the point of the number of cores / amount of parallelism beyond which additional parallelism provides no further benefit or (worse) may reduce performance.
![[maxEffPar.png]]
##### Calculation Example: Maximum Effective Parallelism
- Remember from Amdahl:
- $S = \frac{T_S}{T_P}$ (i.e. sequential time over parallel time)
- You can use these equations to approximate behaviour of modelled systems.
- E.g.: to represent speedup, comms overhead, cost of equipment, etc. and use calculations to estimate optimal selections.
### Parallel Efficiency ($E_\text{par}$)
- We can think of the efficiency of various things, e.g. power systems, motors, heaters etc. We can also think of the efficiency of parallel computing…
- Parallel Efficiency
	- Defined as the:
ratio of speedup (S) to the number of processors (p)
• Parallel Efficiency measures the fraction of time for which a processor is usefully used.
• An efficiency of 1 is ideal (>1 means you may be harnessing energy from another dimension ☺)
$$
E = \frac{S}{p} = \frac{T_s}{pT_p}
$$
- p = number processors
- Ts = exe time of the seq. alg.
- Tp = exe time of the parallel alg. with p processors used
- Sp= speedup (linear being the realistic ideal)
![[Epar.png]]
### Gustafson’s Law *
- Gustafson's law =
- The theoretical speedup in `latency` of the execution of a task at fixed execution time that can be expected of a system whose resources are improved.
- A follow-up to Amdahl's Law
* also referred to, more fairly, as the Gustafson–Barsis's law
Speedup S gained by N processors (instead of just one) for a task with a serial fraction s (not benefiting from parallelism) is as follows:
$$S = N+(1-N)s$$
S = speedup, N = cores, s = serial portion
Using different variables, can be formulated as:
$$S_\text{latency}(s)=1-p+sp$$
$S_\text{latency}$ = theoretical speedup in latency of the execution of the whole task
p = % of workload before the improvement that will be speeded up.
s = speedup in latency of part of the task benefiting from improved parallelism
This is a plot of $S_\text{latency}(s) = 1-p+sp$
s going from 0.5 to 2
p going from 10% to 100%
![[gusLawPlot.png]]
As you can see, the latency is reduced (i.e. $S_\text{latency}$ increased) as s increases and p increases. But for e.g. a real speedup of 2 you actually need p=100% (i.e.
the entire workload improved) to achieve it.
p = % of workload before the improvement that will be speeded up.
s = speedup in latency of part of the task benefiting from improved parallelism
Gustafson’s law can be more useful to real-time embedded systems because it look more towards improving the response time of a system.
