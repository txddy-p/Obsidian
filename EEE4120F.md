![[golden_gate_bridge.png]]
## Golden Measure
- A sequential solution that you develop as the yard stick
- A solution that may run slowly, isn't optimised but you know it gives (numerically speaking) excellent results
- Its not about computation time but accuracy
## Sequential / Serial
A non-parallelized code solution
## Speed-up
- $\text{Speed-up} = \frac{T_{p1}}{T_{p2}}$ . $T_{p1}$ = run-time of original (or non-optimized) program
- $T_{p2}$ = run-time of optimised prograam
- Best practise: run the program more than once so as to warm up the system
<<<<<<< HEAD
=======


12/03/2024
## Verification and Validation (V&V)
- Verification (dev end)
  - “Are we building the product right?”
  - Have we made what we understood we wanted to make?
  - Does the product satisfy its specifications?
- Validation (user end)
  - “Are we building the right product?”
  - Does the product satisfy the users’ requirements
- Verification before validation (at least in duress)…
While it would be nice to validate (seeing that the users are happy) before verifying (checking the specs), doing so would mean your final design might not match the specifications (which could open the door to legal problems). Obviously this often doesn’t happen because in practice you want to make sure the client is happy and there might not be time for proper validation.
## Verification before validation
- The RC engineer (i.e., you) are effectively designing both custom hardware and custom
software for the RC platform
- Before attempting to make claims about the validity of your system, it’s usually best practice to establish your own (or team’s) confidence in what your system is doing, i.e. be sure that:
  - The custom hardware working;
  - The software implementation is doing what it was designed to do; and
  - The custom software runs reliably on the custom hardware.
## Verification
- Checking plans, documents, code, requirements and specifications
- Is everything that you need there?
- Algorithms/functions working properly?
- Done during phase interval (e.g., design => implementation)
- Activities:
  - Review meetings, walkthroughs, inspections
  - Informal demonstrations
## Commonly used verification methods
1. Dual processing, producing two result sets
  1. One version using PC & simulation only;
  2. Other version including RC platform
2. Assume the PC version is the correct one (i.e., the gold measure)
3. Correlate the results to establish correlation coefficients
(complex systems may have many different sets of possibly multidimensional data that need to be compared)
The correlation coefficients can be used as a kind of ‘confidence factor’
## Validation
- Testing of the whole product / system
- Input: checklist of things to test or list of issues that need to have been provided/fixed
- Towards end of project
- Activities:
  - Formal demonstrations
  - Factory Acceptance Test
## Testing and Correctness proofs
- Testing
  - Generally refers to aspects of dynamic validation in which a program is executed and the results analysed
- Correctness proofs / formal verification
  - More a mathematical approach
  - Exhaustive test => specification guaranteed correct
  - Formal verification of hardware is especially relevant to RC. Formal methods include:
    -Model checking / state space exploration
    -Use of linear temporal logic and computational tree logic
    -Mathematical proof (e.g. proof by induction)
## Amdahl’s Law Insert graph on page 15 here

## Essentials of Amdahl’s Law
- Be aware that a computer program to run on a parallel computer * pretty much always has a part that is sequential, which can run on only one core, and a part that is parallel, that can be split between available cores
- But, let’s make things more fun (and hope you then understand Amdahl’s better) by proceeding to video linked on next slide.
- Comments on slide 19 elaborates further.
*well, we’re thinking here computers with one or more CPUs for their processing

## Amdahl’s Law
- Define f as: fraction of computation that can be parallelized (ignoring scheduling overhead)
- Then (1 -f ) is the fraction that is sequential
- Define n = no. processors for parallel case
- The maximum speed-up achievable is:
$$\text{Speedup}_\text{parallel} = \frac{1}{(1-f)+\frac{f}{n}}$$
Should be able to remember this formula for exams
### Alternate Representation
P = expected performance improvement
$E^u$ = Execution time on a uniprocessor (serial)
$E^p$ = Execution time on a number of processors (parallel)
n = number of processors
S = fraction of time spent in the sequential time
$$P = \frac{E^u}{E^p} = \frac{E^u}{SE^u+\frac{(1-s)}{N}E^u} = \frac{1}{s+\frac{1-S}{N}}$$




# Lecture 6
# Computation Methods

| Hardware                                                                                                                                                                                    | Reconfigurable Computer                                                                                                                                                                                                                                    | Software Processor                                                                                                                                                                                                                |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Advantages:<br>• High speed & performance<br>• Efficient (possibly<br>lower power than idle<br>processors)<br>• Parallelizable<br>Drawbacks:<br>• Expensive<br>• Static (cannot change)<br> | e.g. IBM Blade, FPGA-based<br>computing platform<br>Advantages:<br>• Faster than software alone<br>• More flexible than software<br>• More flexible than hardware<br>• Parallelizable<br>Drawbacks:<br>• Expensive<br>• Complex<br>(both s/w<br>& h/w)<br> | e.g. PC, embedded<br>software on<br>microcontroller<br>Advantages:<br>• Flexible<br>• Adaptable<br>• Can be much<br>cheaper<br>Drawbacks:<br>• The hardware is static<br>• Limit of clock speed<br>• Sequential<br>processing<br> |
# Mainstream parallel computing
- Most server class machines today are:
	- PC class SMP’s (Symmetric Multi-Processors *)
	- 2, 4, 8 processors - cheap
	- Run Windows & Linux
- Delux SMP’s
	- 8 to 64 processors
	- Expensive:
	- 16-way SMP costs ≈ 4 x 4-way SMPs
- Applications: Databases, web servers, internet commerce / OLTP (online transaction processing)
- Newer applications: technical computing, threat analysis, credit card fraud...
![[SMP.png]]
* Also termed “Shared Memory Processor” (but you might get 0 for giving this alternate term in a test)
# Large scale parallel computing systems
- Hundreds of processors, typically as SMPs clusters
- Traditionally
	- Often custom built with government funding (costly! 10 to 100 million USD)
	- National / international resource
	- Total sales tiny fraction of PC server sales
- Few independent software developers
- Programmed by small set of majorly smart people
- Later trends
	- Cloud systems
	- Users from all over
	- E.g. Amazon EC, Microsoft Azure
- Some application examples
	- Code breaking (CIA, FBI)
	- Weather and climate modeling / prediction
	- Pharmaceutical – drug simulation, DNA modeling and drug design
	- Scientific research (e.g., astronomy, SETI*)
	- Defense and weapons development
- Large-scale parallel systems are often used for modelling
* This one has changed, SETI@home which used many volunteers across the world doing processing was large scale parallel but not a single computing system… so this example isn’t really valid for this case. Furthermote, SETI@home is currently hibernating.
# Classic techniques for parallel programming*
- Single Program Multiple Data (SPMD)
	- Consider it as running the same program, on different data inputs, on different computers (possibly) at the same time
- Multiple Program Multiple Data (MPMD)
	- Consider this one as running the same program with different parameters settings, or recompiling the same code with different sections of code included (e.g., uisng #ifdef and #endif to do this)
- Following this approach performance statistics can be gathered (without necessarily any parallel code being written) and then evaluated after the effect to deem the feasibility of implementing a parallel (e.g. actual pthreads version) of the program.
Can consider the SPSD (i.e. Single Program Single Data) as the case where the whole program is run once on all the data.
# Terms (reminders)
- Observed speedup =
$$
\frac{\text{Wallclock time initial version
}}{\text{Wallclock time refined version}} =\frac{\text{Wallclock time sequential (or gold)}}{\text{Wallclock time parallel version}}
$$
- Parallel overhead:
- Amount of time to coordinate parallel tasks (excludes time doing useful work).
- Parallel overhead includes operations such as:
	Task/co-processor start-up time, 	Synchronizations, communications,
	parallelization libraries (e.g., OpenMP, 	Pthreads.so), tools, operating system, task 	termination and clean-up time

The parallel overhead of the lazy parallel model could clearly be extreme, considering that is would rely on manual intervention to (e.g.) partition and prepare the data before the program runs.
## Some terms
- Embarrassingly Parallel
	- Simultaneously performing many similar, independent tasks, with little to no coordination between tasks.
- Massively Parallel
	- Hardware that has very many processors (execution of parallel tasks). Can consider this classification of 100 000+ parallel tasks.
- { Stupidly Parallel }
	- While this isn’t really an official term it typically relates to instances where a big (and possibly very complex) coding effort is put into developing a solution that in practice has negligible savings or worse is a whole lot slower (and possibly more erroneous/buggy) than if it was just left as a simpler sequential implementation. 
### Partitioned memory
![[partitioned_memory.png]]
### Interlaced* memory
![[Interlaced_memory.png]]
## Memory Partitioning Terms
![[memoryPartitioningTerm.png]]
## Flynn’s taxonomy
- Flynn’s (1966) taxonomy was developed as a means to classify parallel computer architectures
- Computer system can be fit into one of the following four forms:
![[FlynsTaxonomy.png]]
Not to be confused with the terms of “Single Program Multiple Data (SPMD)” and “Multiple Program Multiple Data (MPMD)” mentioned earlier.
#### Single Instruction Single Data (SISD)
- This is (obviously) the classic von Neumann Computer: serial (not parallel) computer, e.g.:
	- Old style single core PC CPUs, e.g. i486
- Single instruction →
	- One instruction stream acted on by the CPU during any one clock cycle
- Single data →
	- Only one input data stream for any one clock cycle
- Deterministic execution
![[SISD.png]]
#### Single Instruction Multiple Data (SIMD)
- A form of parallel computer
	- Early supercomputers used this model first
	- Nowadays it has become common – e.g., used in modern computers on GPUs
- Single instruction →
	- All processing units execute the same instruction for any given clock cycle
- Multiple data →
	- Each processing unit can operate on a different data element
![[SIMD.png]]
#### Single Instruction Multiple Data (SIMD)
- Runs in lockstep (i.e., all elements synchronized)
- Works well for algorithms with a lot of regularity; e.g. graphics processing.
- Two main types:
	- Processor arrays
	- Vector pipelines
- Still highly deterministic (know the same operation is applied to specific set of data – but more data to keep track of per instruction)
#### Single Instruction Multiple Data (SIMD) Examples
- Vector pipelines
	- IBM 9000, Cray X-MP,
	Fujitsu vector processor,
	NEC SX-2, Hitachi S820, ETA10
- Processor arrays
	- Thinking Machine CM2,
	MasPar MP-1 & MP-2,
	ILLIAC IV
- Graphics processor units usually use SIMD
#### Multiple Instruction Single Data (MISD)
- Single data stream fed into multiple processing units
- Each processing unit works on data independently via independent instruction streams
- Few actual examples of this class of parallel computer have ever existed
#### Multiple Instruction Single Data (MISD) Example
- Possible uses? Somewhat intellectual?
	- Maybe redundant! (see next slide)
- Possible example application:
	- Different set of signal processing operations working the same signal stream
![[simMinMax.png]]
#### Multiple Instruction Multiple Data (MIMD)
- The most common type of parallel computer (most late model computers, e.g. Intel Core Duo, in this category)
- Multiple Instruction →
	- Each processor can be executing a different instruction stream
- Multiple Data →
	- Every processor may be working with a different data stream
- Execution can be asynchronous or synchronous; non-deterministic or deterministic
- Examples
	- Many of the current supercomputers
	- Networked parallel computer clusters
	- SMP computers
	- multi-core PCs
- MIMD architectures could include all the other models. e.g.,
	- SISD – just one CPU active, others running NOP
	- SIMD – all CPUs load the same instruction but apply to different data
	- MISD – all CPUs load different instructions but apply it to the same data
# Maximum Effective Parallelism
## Significant Tradeoff:
### Infrastructure & Setup Cost vs. Effective Parallelism
Consideration for
- hardware
- software
### Maximum Effective Parallelism:
This refers to the point of the number of cores / amount of parallelism
beyond which additional parallelism provides no further benefit or (worse)
may reduce performance.
### Maximum Effective Parallelism:
This refers to the point of the number of cores / amount of parallelism beyond which additional parallelism provides no further benefit or (worse) may reduce performance.
![[maxEffPar.png]]
##### Calculation Example: Maximum Effective Parallelism
- Remember from Amdahl:
- $S = \frac{T_S}{T_P}$ (i.e. sequential time over parallel time)
- You can use these equations to approximate behaviour of modelled systems.
- E.g.: to represent speedup, comms overhead, cost of equipment, etc. and use calculations to estimate optimal selections.
### Parallel Efficiency ($E_\text{par}$)
- We can think of the efficiency of various things, e.g. power systems, motors, heaters etc. We can also think of the efficiency of parallel computing…
- Parallel Efficiency
	- Defined as the:
ratio of speedup (S) to the number of processors (p)
• Parallel Efficiency measures the fraction of time for which a processor is usefully used.
• An efficiency of 1 is ideal (>1 means you may be harnessing energy from another dimension ☺)
$$
E = \frac{S}{p} = \frac{T_s}{pT_p}
$$
- p = number processors
- Ts = exe time of the seq. alg.
- Tp = exe time of the parallel alg. with p processors used
- Sp= speedup (linear being the realistic ideal)
![[Epar.png]]
### Gustafson’s Law *
- Gustafson's law =
- The theoretical speedup in `latency` of the execution of a task at fixed execution time that can be expected of a system whose resources are improved.
- A follow-up to Amdahl's Law
* also referred to, more fairly, as the Gustafson–Barsis's law
Speedup S gained by N processors (instead of just one) for a task with a serial fraction s (not benefiting from parallelism) is as follows:
$$S = N+(1-N)s$$
S = speedup, N = cores, s = serial portion
Using different variables, can be formulated as:
$$S_\text{latency}(s)=1-p+sp$$
$S_\text{latency}$ = theoretical speedup in latency of the execution of the whole task
p = % of workload before the improvement that will be speeded up.
s = speedup in latency of part of the task benefiting from improved parallelism
This is a plot of $S_\text{latency}(s) = 1-p+sp$
s going from 0.5 to 2
p going from 10% to 100%
![[gusLawPlot.png]]
As you can see, the latency is reduced (i.e. $S_\text{latency}$ increased) as s increases and p increases. But for e.g. a real speedup of 2 you actually need p=100% (i.e.
the entire workload improved) to achieve it.
p = % of workload before the improvement that will be speeded up.
s = speedup in latency of part of the task benefiting from improved parallelism
Gustafson’s law can be more useful to real-time embedded systems because it look more towards improving the response time of a system.
# Lecture 7
# Lecture 8
## Toward HPES system design and programming
- We have looked into essential aspects of (single-core) processor design, and thinking about parallel systems.
- Thinking point regards parallel system design:
	- Does it make more sense to start on the design of a parallel computing solution by
	Deciding the parallel platform/hardware first… or
	Deciding the data and processing needs first?
- In this course, the direction is to start thinking about the data and processing, and then elaborating on methods and considering hardware constructs by which these are delivered.
### Steps in Designing Parallel Programs
The hardware may be done first… or later.
The main steps:
1. Understand the problem
2. Partitioning (separation into main tasks)
3. Granularity
4. Communications
5. Identify data dependencies
6. Synchronization
7. Load balancing
8. Performance analysis and tuning
## Part 1/3
### Step 1: Understanding the Problem
- Make sure that you understand the problem, that is the right problem, before you attempt to formulate a solution
- Some problems aren’t suited to parallelization – it just might not be parallelizable
Example of non-parallelizable problem:
Calculating the set of Fibonacci series e.g., $Fib(10^{20})$ as quickly as possible.
```C
Fib(n) = n                       } if n < 2
	   = Fib(n – 1) + Fib(n – 2) } otherwise
```
- Identify: Critical parts or ‘hotspots’
	- Determine where most of the work needs to be done. NB: Most scientific and technical programs accomplish the most substantial portion of the work in only a few small places.
	- Focus on parallelizing hotspots. Ignore parts of the program that don’t need much CPU use.
	- Consider which profiling techniques and Performance Analysis Tools (‘PATs’) to use
- Identify: bottlenecks
	- Consider communication, I/O, memory and processing bottlenecks
	- Determine areas of the code that execute notably slower than others.
	- Add buffers / lists to avoid waiting
	- Attempt to avoid blocking calls (e.g., only block if the output buffer is full)
	- Try to rearrange code to make loops faster
- General method:
	- identify hotspots, avoid unnecessary complication, identify potential inhibitors to the parallel design (e.g., data dependencies).
- Consider other algorithms…
	- This is an most important aspect of designing parallel applications.
	- Sometimes the obvious method can be greatly improved upon though some lateral thought, and testing on paper.
#### Step 1: Identifying the Problem: where the solution may fit…
- This also brings in aspects of Lecture 1, ‘the Landscape of Parallel Computing’*
- Considering the 7 questions:
1. What are the applications?
2. What are the common kernels?
3. What are the hardware building blocks?
4. How to connect them?
5. How to describe allocations and kernels?
6. How to program the hardware?
7. How to measure success?
### Step 2: Partitioning
- This step involves breaking the problem into separate chunks of work, which can then be implemented as multiple distributed tasks.
- Two typical methods to partition computation among parallel tasks:
	1. Functional decomposition or
	2. Domain decomposition
#### Functional decomposition
- Decomposing the problem into tasks to be done.
- Functional decomposition is suited to problems that can be split into different tasks
- Example applications
	- Environment modelling
	- Simulating reality
	- Signal processing, e.g.:
		- Pipelined filters: in → P1 → P2 → P3 → out
		- Here P1 is filled first, its result is sent to P2 while simultaneously P1 starts working on a block of new input, and so on.
	- Climate modelling (e.g., simultaneously running simulations for atmosphere, land, and sea)
#### Domain decomposition
Involves separating the data, or taking different ‘views’ of the same data. e.g.
View 1 : looking at frequencies (e.g. FFTs)
View 2 : looking at amplitudes
Each parallel task works on its own portion of the data, or does something different with the same data
![[DomainDecomp.png]]
- Good to use for problems where:
	- Data is static (e.g., factoring; matrix calculations)
	- Dynamic data structures linked to a single entity (where entity can be made into subsets) (e.g., multi-body problems)
	- Domain is fixed, but computation within certain regions of the domain is dynamic (e.g., fluid vortices models)
Many possible ways to divide things up. If you want to look at it visually…
![[domainDecomplec7.png]]
### Step 3: Granularity
- Granularity of the Problem vs. Granularity of the Parallelism
### Step 3: Granularity of the Problem
- Granularity of the problem
	- How big or small are the parts that the problem has been decomposed into?
	- How interrelated are the sub-tasks
Fine Grained:
	Each calculation highly dependent on other parts of the problem space. Requires a great deal of communication.
Coarse Grained:
	Each calculation largely independent of other parts of the problem space. Requires only a little communication.
#### The Ratio of Computation : Communication
- This ratio can help to decide is a problem is a fine- or coarse-grained problem.
- 1 : 1 = Each intermediate result needs a
communication operation
- 100 : 1 = 100 computations (or intermediate
results) require only one communication
operation
- 1 : 100 = Each computation
needs 100 communication
operations
Decomposition and
Problem Granularity
- Coarse grained:
- Breaking problems into larger pieces
- Usually, low level of task inter-dependence and fewer
interrelations (e.g., can separate into parts whose
elements are unrelated to other parts)
- These solutions are generally easier to parallelize than
fine-grained, and
- Usually, parallelization of these problems provides
significant benefits.
2 1 1 9 3 3
thread
#1
thread
#2 ✓ e.g. sequence search to find a
number followed by its square root.
e.g. Computation:Communication = 100:1
(note: we’re assuming sqrt will take much computation)
✓
Decomposition and Granularity
- Fine-grained problem:
- Problem broken into pieces that have high
levels of inter-dependence. Or difficult to
partition the data because each result is
dependent on much of the available data.
e.g.: having to look at relations
between neighboring dust
particles to determine how a
dust cloud behaves.
e.g. Computation:Communication = 1:100
let’s next consider:
granularity of problem vs
granularity of parallelism…
Example of fine-grained problem,
coarse-grained parallelism
- Fine-grained problem with coarse-grained
parallelism:
- Result(s) of the computation are highly dependent
on much of the data… but can be done as many
parallel operations.
- Example
- Dot-product of two vectors : result is a single scalar
value – so overall obtaining result is fine-grained
problem as it depends on all the other values and
their ordering, but can be sub-divided into many
smaller tasks that provide partial results, so has
coarse-grained parallelism.
a .
b = a1 x b1 + a2 x b2 + … an x bn
2 1 1 9 3 3
2 1 1 9 3 3
The result, a single value in this case, is fine-
grained, depending on all the available data
(but the computation is fairly course-grained, can be split-
up into sub-tasks of ai x bi which are then summed)
Example of Coarse-grained
- Course-grained
- Result(s) of the computation dependent on
only a small portion of the data
- Example
- Increment each element in a vector.a + 1
= [
a1 + 1,
+ a2 + 1,
… an + 1]
Each result, which happens to be
n results in
this case, depends on only one item of data.
Decomposition and Granularity
- Embarrassingly Parallel:
- So coarse that there’s no or very little
interrelation between parts/sub-processes
2 3 2 1 4 0
1 2 3 4 3 2
0
1 0 3 2 4 1
2 2 0 3 4 1
e.g. double all
the entries in a
matrix.
1 2 3 4 3
4 6 4 2 8 0
2 4 6 8 6 4
0
2 0 6 4 8 2
4 4 0 6 8 2
2 4 6 8 6
e.g. Computation:Communication = 1:0
NB: also called “embarrassingly fine-grained parallel” – about to hear why…
Granularity of Parallelism
- Granularity of parallelism:
- How small and plentiful are the tasks (computing parts
or threads) the program is broken into.
- It is more about the implementation.
- Is not answering “how interrelated are the
computations”
- Is about: how small are the pieces of computing
Granularity of ParallelismGranularity of Problem
Homework Task / Class Activity
- Which of the following are more fine-
grained problems, and which are more
coarse-grained problems?
- Matrix multiply
- FFTs
- Decryption code breaking
- (deterministic) Finite state machine validation
/ termination checking
- Map navigation (e.g., shortest path)
- Population modelling
Supplementary
reading
Step 5: Identify Data
Dependencies
EEE4120F
In dog-think: I’m not
budging from here
until I’m done.
Woof!
Comments on syllabus re Part 2:
Data Dependencies, and designing the most efficient ways to work around
these can be very involved. Since the focus of this course is towards
specialized / embedded processing system, the aspects of data
dependencies have been consolidated. Thus, only the essentials of data
dependencies are covered here. Also, keep in mind that most of this is
about parallel programming, but for HPES there is the added complication
that the ‘variables’ are not necessarily (as in a standard parallel processing)
in one place, or used only on one processor (clearly caches and shadow
registers, common to parallel programming, would be useful, but the
caching or registers that are mirrored or protected with semaphores and are
not necessarily controlled by one processor or MMU*).
* MMU = memory management unit
Supplementary
reading
Data dependencies
- A dependency exists between statements
of a program when the order of
executing the statements affects the
results of the program.
- A data dependency is caused by different
tasks accessing the same variables (i.e.,
memory addresses).
- Dependencies are a major inhibition to
developing parallel programs.
: marks key point(s)
Data dependencies
- Common approaches to working around
data dependencies:
- For distributed memory architectures: tend to
use synchronization points (periods when
sets of shared data is communicated
between tasks).
- Shared memory architectures: make use of
read/write synchronize operations (no
sending of data, just temporarily block other
tasks from reading/writing a variable).
Common data dependencies
• Loop carried data dependence:
dependence between statements in different iterations
• Loop independent data dependence:
dependence between statements in the same iteration
• Lexically forward dependence:
source precedes the target lexically
• Lexically backward dependence:
opposite from above
• Right-hand side of an assignment precede the left-hand
side
Source: http://sc.tamu.edu/help/power/powerlearn/html/ScalarOptnw/tsld036.htm
1. Understand the problem
2. Partitioning
3. Granularity
4. ….
5. Identify data dependencies
Part 1 Done
Over to you to work on OpenCL prac or have an early start on Prac3
Further Reading / Refs
• http://unilearning.uow.edu.au/critical/1a.html
Some resources related to critical analysis and critical thinking:
• http://www.deakin.edu.au/current-students/study-support/study-
skills/handouts/critical-analysis.php
An easy introduction to critical analysis:
An online quiz and learning tool for understanding critical thinking:
Some resources related to systems thinking:
• https://www.youtube.com/watch?v=lhbLNBqhQkc
• https://www.youtube.com/watch?v=AP7hMdnNrH4
Suggestions for further reading, slides partially based / general principles
elaborated in:
• https://computing.llnl.gov/tutorials/parallel_comp/
• http://www2.physics.uiowa.edu/~ghowes/teach/ihpc12/lec/ihpc12Lec_Desig
nHPC12.pdf
Supplementary
reading
Image sources:
Clipart sources – public domain CC0 (http://pixabay.com/)
PxFuel – CC0 (https://www.pxfuel.com/)
Pixabay
commons.wikimedia.org
Images from flickr
Disclaimers and copyright/licensing details
I have tried to follow the correct practices concerning copyright and licensing of material,
particularly image sources that have been used in this presentation. I have put much
effort into trying to make this material open access so that it can be of benefit to others in
their teaching and learning practice. Any mistakes or omissions with regards to these
issues I will correct when notified. To the best of my understanding the material in these
slides can be shared according to the Creative Commons “Attribution-ShareAlike 4.0
International (CC BY-SA 4.0)” license, and that is why I selected that license to apply to
this presentation (it’s not because I particulate want my slides referenced but more to
acknowledge the sources and generosity of others who have provided free material such
as the images I have used).