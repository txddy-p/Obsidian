![[golden_gate_bridge.png]]
## Golden Measure
- A sequential solution that you develop as the yard stick
- A solution that may run slowly, isn't optimised but you know it gives (numerically speaking) excellent results
- Its not about computation time but accuracy
## Sequential / Serial
A non-parallelized code solution
## Speed-up
- $\text{Speed-up} = \frac{T_{p1}}{T_{p2}}$ . $T_{p1}$ = run-time of original (or non-optimized) program
- $T_{p2}$ = run-time of optimised prograam
- Best practise: run the program more than once so as to warm up the system
<<<<<<< HEAD
=======


12/03/2024
## Verification and Validation (V&V)
- Verification (dev end)
  - “Are we building the product right?”
  - Have we made what we understood we wanted to make?
  - Does the product satisfy its specifications?
- Validation (user end)
  - “Are we building the right product?”
  - Does the product satisfy the users’ requirements
- Verification before validation (at least in duress)…
While it would be nice to validate (seeing that the users are happy) before verifying (checking the specs), doing so would mean your final design might not match the specifications (which could open the door to legal problems). Obviously this often doesn’t happen because in practice you want to make sure the client is happy and there might not be time for proper validation.
## Verification before validation
- The RC engineer (i.e., you) are effectively designing both custom hardware and custom
software for the RC platform
- Before attempting to make claims about the validity of your system, it’s usually best practice to establish your own (or team’s) confidence in what your system is doing, i.e. be sure that:
  - The custom hardware working;
  - The software implementation is doing what it was designed to do; and
  - The custom software runs reliably on the custom hardware.
## Verification
- Checking plans, documents, code, requirements and specifications
- Is everything that you need there?
- Algorithms/functions working properly?
- Done during phase interval (e.g., design => implementation)
- Activities:
  - Review meetings, walkthroughs, inspections
  - Informal demonstrations
## Commonly used verification methods
1. Dual processing, producing two result sets
  1. One version using PC & simulation only;
  2. Other version including RC platform
2. Assume the PC version is the correct one (i.e., the gold measure)
3. Correlate the results to establish correlation coefficients
(complex systems may have many different sets of possibly multidimensional data that need to be compared)
The correlation coefficients can be used as a kind of ‘confidence factor’
## Validation
- Testing of the whole product / system
- Input: checklist of things to test or list of issues that need to have been provided/fixed
- Towards end of project
- Activities:
  - Formal demonstrations
  - Factory Acceptance Test
## Testing and Correctness proofs
- Testing
  - Generally refers to aspects of dynamic validation in which a program is executed and the results analysed
- Correctness proofs / formal verification
  - More a mathematical approach
  - Exhaustive test => specification guaranteed correct
  - Formal verification of hardware is especially relevant to RC. Formal methods include:
    -Model checking / state space exploration
    -Use of linear temporal logic and computational tree logic
    -Mathematical proof (e.g. proof by induction)
## Amdahl’s Law Insert graph on page 15 here

## Essentials of Amdahl’s Law
- Be aware that a computer program to run on a parallel computer * pretty much always has a part that is sequential, which can run on only one core, and a part that is parallel, that can be split between available cores
- But, let’s make things more fun (and hope you then understand Amdahl’s better) by proceeding to video linked on next slide.
- Comments on slide 19 elaborates further.
*well, we’re thinking here computers with one or more CPUs for their processing

## Amdahl’s Law
- Define f as: fraction of computation that can be parallelized (ignoring scheduling overhead)
- Then (1 -f ) is the fraction that is sequential
- Define n = no. processors for parallel case
- The maximum speed-up achievable is:
$$\text{Speedup}_\text{parallel} = \frac{1}{(1-f)+\frac{f}{n}}$$
Should be able to remember this formula for exams
### Alternate Representation
P = expected performance improvement
$E^u$ = Execution time on a uniprocessor (serial)
$E^p$ = Execution time on a number of processors (parallel)
n = number of processors
S = fraction of time spent in the sequential time
$$P = \frac{E^u}{E^p} = \frac{E^u}{SE^u+\frac{(1-s)}{N}E^u} = \frac{1}{s+\frac{1-S}{N}}$$




# Lecture 6
# Computation Methods

| Hardware                                                                                                                                                                                    | Reconfigurable Computer                                                                                                                                                                                                                                    | Software Processor                                                                                                                                                                                                                |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Advantages:<br>• High speed & performance<br>• Efficient (possibly<br>lower power than idle<br>processors)<br>• Parallelizable<br>Drawbacks:<br>• Expensive<br>• Static (cannot change)<br> | e.g. IBM Blade, FPGA-based<br>computing platform<br>Advantages:<br>• Faster than software alone<br>• More flexible than software<br>• More flexible than hardware<br>• Parallelizable<br>Drawbacks:<br>• Expensive<br>• Complex<br>(both s/w<br>& h/w)<br> | e.g. PC, embedded<br>software on<br>microcontroller<br>Advantages:<br>• Flexible<br>• Adaptable<br>• Can be much<br>cheaper<br>Drawbacks:<br>• The hardware is static<br>• Limit of clock speed<br>• Sequential<br>processing<br> |
# Mainstream parallel computing
- Most server class machines today are:
	- PC class SMP’s (Symmetric Multi-Processors *)
	- 2, 4, 8 processors - cheap
	- Run Windows & Linux
- Delux SMP’s
	- 8 to 64 processors
	- Expensive:
	- 16-way SMP costs ≈ 4 x 4-way SMPs
- Applications: Databases, web servers, internet commerce / OLTP (online transaction processing)
- Newer applications: technical computing, threat analysis, credit card fraud...
![[SMP.png]]
* Also termed “Shared Memory Processor” (but you might get 0 for giving this alternate term in a test)
# Large scale parallel computing systems
- Hundreds of processors, typically as SMPs clusters
- Traditionally
	- Often custom built with government funding (costly! 10 to 100 million USD)
	- National / international resource
	- Total sales tiny fraction of PC server sales
- Few independent software developers
- Programmed by small set of majorly smart people
- Later trends
	- Cloud systems
	- Users from all over
	- E.g. Amazon EC, Microsoft Azure
- Some application examples
	- Code breaking (CIA, FBI)
	- Weather and climate modeling / prediction
	- Pharmaceutical – drug simulation, DNA modeling and drug design
	- Scientific research (e.g., astronomy, SETI*)
	- Defense and weapons development
- Large-scale parallel systems are often used for modelling
* This one has changed, SETI@home which used many volunteers across the world doing processing was large scale parallel but not a single computing system… so this example isn’t really valid for this case. Furthermote, SETI@home is currently hibernating.
# Classic techniques for parallel programming*
- Single Program Multiple Data (SPMD)
	- Consider it as running the same program, on different data inputs, on different computers (possibly) at the same time
- Multiple Program Multiple Data (MPMD)
	- Consider this one as running the same program with different parameters settings, or recompiling the same code with different sections of code included (e.g., uisng #ifdef and #endif to do this)
- Following this approach performance statistics can be gathered (without necessarily any parallel code being written) and then evaluated after the effect to deem the feasibility of implementing a parallel (e.g. actual pthreads version) of the program.
Can consider the SPSD (i.e. Single Program Single Data) as the case where the whole program is run once on all the data.
# Terms (reminders)
- Observed speedup =
$$
\frac{\text{Wallclock time initial version
}}{\text{Wallclock time refined version}} =\frac{\text{Wallclock time sequential (or gold)}}{\text{Wallclock time parallel version}}
$$
- Parallel overhead:
- Amount of time to coordinate parallel tasks (excludes time doing useful work).
- Parallel overhead includes operations such as:
	Task/co-processor start-up time, 	Synchronizations, communications,
	parallelization libraries (e.g., OpenMP, 	Pthreads.so), tools, operating system, task 	termination and clean-up time

The parallel overhead of the lazy parallel model could clearly be extreme, considering that is would rely on manual intervention to (e.g.) partition and prepare the data before the program runs.
## Some terms
- Embarrassingly Parallel
	- Simultaneously performing many similar, independent tasks, with little to no coordination between tasks.
- Massively Parallel
	- Hardware that has very many processors (execution of parallel tasks). Can consider this classification of 100 000+ parallel tasks.
- { Stupidly Parallel }
	- While this isn’t really an official term it typically relates to instances where a big (and possibly very complex) coding effort is put into developing a solution that in practice has negligible savings or worse is a whole lot slower (and possibly more erroneous/buggy) than if it was just left as a simpler sequential implementation. 
## Partitioned memory
[a1 a2 a1 a2 … am-1] [b1 b2 b1 b2 … bm-1]
Thread
1 sum1
Vectors in
global / shared
memory
[am am+1 am+2 … an] [bm bm+1 bm+2 … bn]
Thread
2 sum2
starts
main()
sum =
sum1+sum2
Assuming a 2-core machine
1 1
2 2
a b
main()
Memory access
by thread
Interlaced* memory
[a1 a3 a5 a7 … an-2] [b1 b3 b5 b7 … bn-2]
Thread
1 sum1
Vectors in
global / shared
memory
[a2 a4 a6 … an-1] [b2 b4 b6 … bn-1]
Thread
2 sum2
starts
main()
sum =
sum1+sum2
Assuming a 2 core machine
a b
* ‘Data striping’, ‘interleaving’ and ‘interlacing’ usually means the same thing but not always.
See further explanation between these on: https://wikidiff.com/interleave/interlace
main()
Memory access
by thread
NB: benefits of interleaving as a means to improve robustness and access of memory
interlacing vs. interleaving : see comments
Memory Partitioning Terms
Contiguous
Partitioned (or separated or split)
Interleaved/interlaced (or alternating or data striping)
Interleaved – large strides (e.g. row of image pixels at a time)
Interleaved – small stride (e.g. one word stride)
(the ‘stride’ in data interleaving or data striping refers to the size of the blocks alternated, generally
this is fixed but could be changeable, e.g. the last stride might be smaller to avoid padding data.)
Flynns Taxonomy of Processor
Architectures
EEE4120F
Type
A
Type
B
Type
C
Type
D
Type
E
Type
F
Type
G
Type
H
Type
I
Type
J
Flynn’s taxonomy
- Flynn’s (1966) taxonomy was developed
as a means to classify parallel computer
architectures
- Computer system can be fit into one of
the following four forms:
SISD
Single Instruction
Single Data
SIMD
Single Instruction
Multiple Data
MISD
Multiple Instructions
Single Data
MIMD
Multiple Instructions
Multiple Data
Not to be confused with the terms of “Single Program Multiple Data (SPMD)” and
“Multiple Program Multiple Data (MPMD)” mentioned earlier.
MI
MD
Single Instruction Single Data
(SISD)
- This is (obviously) the classic von Neumann
Computer: serial (not parallel) computer, e.g.:
- Old style single core PC CPUs, e.g. i486
- Single instruction →
- One instruction stream acted on by
the CPU during any one clock cycle
- Single data →
- Only one input data stream for any
one clock cycle
- Deterministic execution
0x1000 LD A,[0x2002]
0x1003 LD B,[0x2004]
0x1006 ADD A,B
0x1007 SHL A,1
0x1008 ST A,[0x2000]
x = 2 * (y + z);
Single Instruction Multiple Data
(SIMD)
- A form of parallel computer
- Early supercomputers used this model first
- Nowadays it has become common – e.g., used in
modern computers on GPUs
- Single instruction →
- All processing units execute the
same instruction for any given
clock cycle
- Multiple data →
- Each processing unit can
operate on a different data
element
LD AX,[DX+0]
LD BX,[EX+0]
ADD AX,BX
SHL AX,1
ST AX,[CX+0]
y= [1 2 3 4]
z = [2 3 4 5]
x = 2 * (y + z)
…
LD AX,[DX+3]
LD BX,[EX+3]
ADD AX,BX
SHL AX,1
ST AX,[CX+3]
…
…
CPU 1 CPU 4
Single Instruction Multiple Data
(SIMD)
- Runs in lockstep (i.e., all elements
synchronized)
- Works well for algorithms with a lot of
regularity; e.g. graphics processing.
- Two main types:
- Processor arrays
- Vector pipelines
- Still highly deterministic (know the same
operation is applied to specific set of data – but
more data to keep track of per instruction)
Single Instruction Multiple Data
(SIMD) Examples
- Vector pipelines
- IBM 9000, Cray X-MP,
Fujitsu vector processor,
NEC SX-2, Hitachi S820, ETA10
- Processor arrays
- Thinking Machine CM2,
MasPar MP-1 & MP-2,
ILLIAC IV
- Graphics processor units usually use SIMD
Cray X-MP
MasPar MP-1
Multiple Instruction Single Data
(MISD)
- Single data stream fed into multiple
processing units
- Each processing unit works on data
independently via independent
instruction streams
- Few actual examples of this class of
parallel computer have ever existed
………
Multiple Instruction Single Data
(MISD) Example
- Possible uses? Somewhat intellectual?
- Maybe redundant! (see next slide)
- Possible example application:
- Different set of signal processing operations working
the same signal stream
x = +MAXINT
Example:
Simultaneously find the min and max input, and do a sum of inputs.
x = -MAXINT x = 0
If A<x then x = A If A>x then x =
A
x = x + A
A = input
CPU 1 CPU 2 CPU 3
Multiple Instruction Multiple Data
(MIMD)
- The most common type of parallel computer (most
late model computers, e.g. Intel Core Duo, in this
category)
- Multiple Instruction →
- Each processor can be executing a different instruction
stream
- Multiple Data →
- Every processor may be working with a different data stream
- Execution can be asynchronous or synchronous; non-
deterministic or deterministic
Multiple Instruction Multiple Data
(MIMD)
- Examples
- Many of the current supercomputers
- Networked parallel computer clusters
- SMP computers
- multi-core PCs
- MIMD architectures could include
all the other models. e.g.,
- SISD – just one CPU active, others running NOP
- SIMD – all CPUs load the same instruction but
apply to different data
- MISD – all CPUs load different instructions
but apply it to the same data
AMD Opteron
IBM BlueGene
Maximum Effective Parallelism
EEE4120F
Significant Tradeoff:
Infrastructure & Setup Cost
vs.
Effective Parallelism
Consideration for
- hardware
- software
Maximum Effective Parallelism:
This refers to the point of the number
of cores / amount of parallelism
beyond which additional parallelism
provides no further benefit or (worse)
may reduce performance.
go parallel!
Significant Tradeoff:
Infrastructure & Setup Cost
vs.
Effective Parallelism
Maximum Effective Parallelism:
This refers to the point of the number of
cores / amount of parallelism beyond which
additional parallelism provides no further
benefit or (worse) may reduce performance.
cores / available parallelism
Performance
improvement
(speedup)
cores / available parallelism
cost of parallelism
cores / available parallelism
startup time
Hardware Software
cores / available parallelism
comms cost and time
Calculation Example:
Maximum Effective Parallelism
- Remember from Amdahl:
- S = TS / TP (i.e. sequential time over parallel time)
- You can use these equations to
approximate behaviour of modelled
systems.
- E.g.: to represent speedup, comms
overhead, cost of equipment, etc. and use
calculations to estimate optimal selections.
(simple example to follow)
Learning Activity
- Quick estimation for maximum effective
parallelism
Solution follows shortly! Please try it for your self, or work with a buddy to think
about how to respond to this question.
Learning Activity Answer
N Proc (ms) Comms (ms) Total (ms)
1 100.000 0 100.000
2 50.000 10 60.000
4 25.000 20 45.000
8 12.500 40 52.500
16 6.250 80 86.250
32 3.125 160 163.125
64 1.563 320 321.563
Could do some rough
calculations (or use
binary search)…
check:
N=2,
N=64,
N=8,
N=4 ✓
Multi processors and sequential
setup time
2nd part of Amdahl’s law video
Amdahl2.flv
Understanding Parallel Computing (Part 2): The Lawn Mower Law LinuxMagazine
If you haven’t already watched this please do!
Parallel Efficiency (Epar)
- We can think of the efficiency of various
things, e.g. power systems, motors, heaters
etc. We can also think of the efficiency of
parallel computing…
- Parallel Efficiency
- Defined as the:
ratio of speedup (S) to the number of processors (p)
• Parallel Efficiency measures the fraction of time
for which a processor is usefully used.
• An efficiency of 1 is ideal (>1 means you may be
harnessing energy from another dimension ☺)
Parallel Efficiency (Epar)
Sp
p (number cores)
sub-linear speedup
linear speedup
super-linear
speedup
(awesome!)
Parallel Efficiency
• p = number processors
• Ts = exe time of the seq. alg.
• Tp = exe time of the parallel alg.
with p processors used
• Sp= speedup
(linear being the realistic ideal)
Gustafson’s Law *
- Gustafson's law =
- The theoretical speedup in latency of the execution
of a task at fixed execution time that can be
expected of a system whose resources are improved.
- A follow-up to Amdahl's Law
* also referred to, more fairly, as the Gustafson–Barsis's law
Speedup S gained by N processors (instead of just one) for a task with
a serial fraction s (not benefiting from parallelism) is as follows:
formulation:
S = speedup, N = cores, s = serial portion
Using different variables, can be formulated as:
Slatecy = theoretical speedup in latency of the execution of the whole task
p = % of workload before the improvement that will be speeded up.
s = speedup in latency of part of the task benefiting from improved parallelism
Gustafson’s Law This is a plot of
s going from 0.5 to 2
p going from 10% to 100%
As you can see, the latency is
reduced (i.e. Slatency increased)
as s increases and p increases.
But for e.g. a real speedup of 2
you actually need p=100% (i.e.
the entire workload improved) to
achieve it.
0
0.5
1
1.5
2
2.5
0 0.2 0.4 0.6 0.8 1 1.2
speedup in latency (slatency)
percentage of workload (p %)
Speedup in Latency (Slatency)
Sl_0.25 Sl_0.50 Sl_1.00 Sl_1.25 Sl_1.50 Sl_2.00
s = 2
s = 1.5
s = 1.25
s = 1
s = 0.5
s = 0.25
p = % of workload before the
improvement that will be speeded up.
s = speedup in latency of part of the task
benefiting from improved parallelism
Gustafson’s law can be more useful to real-time
embedded systems because it look more towards
improving the response time of a system.
End of Lecture
FREE Creative Commons License
COUNTRY BOY
Music: https://www.bensound.com
Image sources:
Clipart sources – public domain CC0 (http://pixabay.com/)
PxFuel – CC0 (https://www.pxfuel.com/)
FreeSVG – CC0 public dimain images (https://freesvg.org/)
Pixabay
commons.wikimedia.org
Images from flickr



